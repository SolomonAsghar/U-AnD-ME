{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7353e0b8-43c1-4d40-997a-4592f9abf770",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15639d45-8457-4bf4-bdf4-bef350bda48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import sys\n",
    "path_to_software_folder = sys.path[0][:-10] + 'software/'\n",
    "sys.path.append(path_to_software_folder)\n",
    "from utils import *\n",
    "from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d671fa6-a3af-4f11-9ae4-5a475262c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FixPredictions(prediction, mode='single'):\n",
    "    \"Enforce physical limits on the prediction\"\n",
    "    \n",
    "    if mode == 'single':\n",
    "        alphas = prediction[0]\n",
    "        Ks = prediction[1]\n",
    "        Ms = np.round(prediction[2])\n",
    "    elif mode =='batch':\n",
    "        alphas = prediction[:,0]\n",
    "        Ks = prediction[:,1]\n",
    "        Ms = np.round(prediction[:,2])\n",
    "    \n",
    "    \n",
    "    alphas = np.where(alphas > 0, alphas, 0)\n",
    "    alphas = np.where(alphas < 1.999, alphas, 1.999)\n",
    "    \n",
    "    Ks = np.where(Ks > 1e-12, Ks, 1e-12)\n",
    "    Ks = np.where(Ks < 1e6, Ks, 1e6)\n",
    "    \n",
    "    Ms = np.where(Ms > 0, Ms, 0)    \n",
    "    Ms = np.where(Ms < 3, Ms, 3)\n",
    "    Ms = np.where(alphas > 1.9, 3, Ms)    # if alpha is over 1.9, M must be 3\n",
    "    \n",
    "    if mode == 'single':\n",
    "        fixed_prediction = np.array([alphas, Ks, Ms])\n",
    "    elif mode =='batch':\n",
    "        fixed_prediction = np.stack([alphas, Ks, Ms], axis=1)\n",
    "    return fixed_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4887f-3091-44d7-afdb-8de92066a69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "\n",
    "def AnalyseEnsembleProperties(Data):\n",
    "    '''\n",
    "    Returns optimal number of components, along with means and stds.\n",
    "    '''\n",
    "    Data = FixPredictions(Data, mode='batch')\n",
    "    \n",
    "    n_components = np.arange(1, 5)[:len(Data)]\n",
    "    models = [mixture.GaussianMixture(n, covariance_type='diag', tol=0.0001, max_iter=1000, n_init=3).fit(Data)\n",
    "              for n in n_components]\n",
    "\n",
    "    BICs = np.zeros(len(n_components))\n",
    "    OverlapFree = np.zeros(len(n_components))\n",
    "    GoodWeights = np.zeros(len(n_components))\n",
    "\n",
    "    for idx, model in enumerate(models):\n",
    "        BICs[idx] = model.bic(Data) \n",
    "        OverlapFree[idx] = CheckGaussianOverlap(model.means_[:,:2], model.covariances_[:,:2], std_scale=0.5)\n",
    "        GoodWeights[idx] = CheckWeights(model.weights_)\n",
    "        \n",
    "    Good_Models = np.logical_and(OverlapFree, GoodWeights)\n",
    "    best_model_index = np.argmin(BICs[Good_Models!=0])\n",
    "    best_model = models[best_model_index]\n",
    "    \n",
    "    opt_num_components = n_components[best_model_index]\n",
    "    means = best_model.means_[:,:2]\n",
    "    stds = best_model.covariances_[:,:2]\n",
    "    weights = best_model.weights_\n",
    "    \n",
    "    return opt_num_components, means, stds, weights, best_model \n",
    "\n",
    "\n",
    "def CheckGaussianOverlap(means, covariances, std_scale=1):\n",
    "    '''\n",
    "    Checks if any Gaussians mean+- (std_scale) standard deviation(s) contains any other Gaussians' means.\n",
    "    Returns True is the GMM is good, False if the GMM is bad.\n",
    "    '''\n",
    "    standard_deviations = np.sqrt(covariances)\n",
    "\n",
    "    all_means_min_scaled_std = means - (standard_deviations*std_scale)\n",
    "    all_means_plu_scaled_std = means + (standard_deviations*std_scale)\n",
    "\n",
    "    for mean in means:\n",
    "        others_mask = means != mean\n",
    "        above_min_mask = all_means_min_scaled_std < mean\n",
    "        below_max_mask = all_means_plu_scaled_std > mean\n",
    "\n",
    "        in_std_range_mask = np.logical_and(above_min_mask, below_max_mask)\n",
    "        other_means_in_std_range = in_std_range_mask[others_mask]\n",
    "\n",
    "        if np.any(other_means_in_std_range):    # if any of the other Gaussians have means within one std of, this GMM is bad\n",
    "            return False\n",
    "    \n",
    "    return True    # if not, this GMM is good!\n",
    "\n",
    "\n",
    "def CheckWeights(weights, weight_cutoff=0.05):\n",
    "    '''\n",
    "    Checks the values of the weights, removes them if they seem too low\n",
    "    '''\n",
    "    return np.all(weights > weight_cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee914b5-9ee4-409b-ba2f-328399767af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PredictAndSplit_M(model, padded_trajs, padding_mask, min_peak_height=0.25):\n",
    "    '''\n",
    "    Predict based on all the padded trajectories passed, then split according to paddding mask and the predicted CPs\n",
    "    '''\n",
    "    ## Make predictions ##\n",
    "    Pred_Labs = model.predict(padded_trajs)\n",
    "    Pred_Labs = np.concatenate(Pred_Labs, axis=2)    # concatenate all label data\n",
    "    ## /Make predictions ##\n",
    "\n",
    "    ## Split up the segments, keeping track of where each came from ##\n",
    "    Label_Segments = []    # we will collect all the segments for each traj into this list\n",
    "    All_CPs = []\n",
    "    \n",
    "    for traj_idx, (traj, pred_lab) in enumerate(zip(padded_trajs, Pred_Labs)):\n",
    "        ## Undo the padding ##\n",
    "        pred_lab = pred_lab[padding_mask[traj_idx]]\n",
    "        ## /Undo the padding ##\n",
    "\n",
    "        CP_labels = pred_lab[:,0]\n",
    "        alpha_and_K_and_class_and_model_each_timestep = pred_lab[:,1:]\n",
    "\n",
    "        ## Get Changepoints ##\n",
    "        CPs = LabelToCP(CP_labels, min_peak_height=min_peak_height)\n",
    "        All_CPs = All_CPs + [np.concatenate((CPs, [np.count_nonzero(padding_mask[traj_idx])]))]    # append lenght as a final CP\n",
    "        ## /Get Changepoints ##\n",
    "\n",
    "        ## Split according to changepoitns ##\n",
    "        pred_label_segments = np.split(alpha_and_K_and_class_and_model_each_timestep, CPs)\n",
    "\n",
    "        ## Save each of these created split segment labels! ##\n",
    "        Label_Segments = Label_Segments + [pred_label_segments]\n",
    "        ## /Split according to changepoitns ##\n",
    "    ## /Split up the segments, keeping track of where each came from ##\n",
    "\n",
    "    return Label_Segments, All_CPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba9576e-0eb4-44bf-9940-8999fdc69724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PhaseOnePredictions(model, data_path, max_traj_len=200, min_peak_height=0.25):\n",
    "    '''\n",
    "    Load all data and make predictions using a U-Net\n",
    "    '''\n",
    "    ### Load data and prepare it for network ###\n",
    "    All_Trajs = []    # stores all the trajs across all exps and fovs!\n",
    "    All_Padding_Masks = []    # stores all the padding masks across all exps and fovs!\n",
    "    All_Traj_Addresses = []    # for each traj, stores what exp and fov its from!\n",
    "\n",
    "    num_exps = len(os.listdir(data_path + '/track_2/'))\n",
    "    for exp in range(num_exps):\n",
    "        all_files = os.listdir(data_path + f'/track_2/exp_{exp}/')\n",
    "        num_fovs = len([fov for fov in all_files if fov.startswith('trajs_fov')])\n",
    "        for fov in range(num_fovs):\n",
    "            FOV_df = pd.read_csv(data_path + f'track_2/exp_{exp}/trajs_fov_{fov}.csv')\n",
    "            FOV = FOV_df.to_numpy()\n",
    "\n",
    "            num_trajs = int(FOV[-1,0]) + 1\n",
    "            all_trajs = np.zeros((num_trajs,max_traj_len,2))   # prepare a container for all the trajs\n",
    "            traj_idx = -1\n",
    "            padding_mask = np.full((num_trajs,max_traj_len), True)    # keeps track of what is padded vs authentic data\n",
    "\n",
    "            _, first_idx = np.unique(FOV[:,0], return_index=True)    # split into diff trajs\n",
    "            split_trajs = np.split(FOV, first_idx[1:])    \n",
    "\n",
    "            for traj in split_trajs:\n",
    "                traj_idx += 1\n",
    "                first_frame, last_frame = int(traj[0,1]), int(traj[-1,1])\n",
    "\n",
    "                all_trajs[traj_idx][first_frame:last_frame+1] = traj[:,2:4]    # drop in the traj\n",
    "                all_trajs[traj_idx][:first_frame] = traj[0,2:4]    # pad the traj!\n",
    "                all_trajs[traj_idx][last_frame+1:] = traj[-1,2:4]\n",
    "\n",
    "                padding_mask[traj_idx][:first_frame] = False    # keep track of what values are padding\n",
    "                padding_mask[traj_idx][last_frame+1:] = False\n",
    "\n",
    "                All_Traj_Addresses = All_Traj_Addresses + [[exp, fov]]\n",
    "\n",
    "            All_Trajs = All_Trajs + [all_trajs]\n",
    "            All_Padding_Masks = All_Padding_Masks + [padding_mask]\n",
    "    All_Trajs = np.concatenate(All_Trajs, axis=0) \n",
    "    All_Trajs = DiffTrajs(All_Trajs)\n",
    "    All_Padding_Masks = np.concatenate(All_Padding_Masks, axis=0)        \n",
    "    All_Traj_Addresses = np.array(All_Traj_Addresses)\n",
    "    ### /Load data and prepare it for network ###\n",
    "\n",
    "    # ## Make predictions ###        \n",
    "    Label_Segments, All_CPs = PredictAndSplit_M(model, All_Trajs, All_Padding_Masks, min_peak_height=min_peak_height)\n",
    "    \n",
    "    return All_Traj_Addresses, Label_Segments, All_CPs, num_exps, num_fovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ecd55-9b43-44a1-a023-43ca67e294e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModel(exp_model):\n",
    "    '''\n",
    "    Convert one hot M prediction to model label\n",
    "    '''\n",
    "    models_possible = ['single_state', 'multi_state', 'dimerization', 'confinement', 'immobile_traps']\n",
    "    model_idx = np.argmax(exp_model)\n",
    "    return models_possible[model_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f36f9b-f2ee-4853-83bb-68127beccfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PhaseTwoPredictions(All_Traj_Addresses, Label_Segments, CPs, \n",
    "                        num_exps, num_fovs,\n",
    "                        output_name,\n",
    "                        predict_ensemble_properties=True):\n",
    "    '''\n",
    "    Take the outputs from phase one and use them to make phase two predictions.\n",
    "    '''    \n",
    "    if predict_ensemble_properties:\n",
    "        ALL_EXP_LABELS = []\n",
    "    for exp in tqdm(range(num_exps)):\n",
    "        ### Collect all segments for this experiment ###\n",
    "        exp_mask = All_Traj_Addresses[:,0] == exp\n",
    "        exp_Traj_Addresses = All_Traj_Addresses[exp_mask]\n",
    "        exp_Labels = [seg_lab for e_mask, seg_lab in zip(exp_mask, Label_Segments) if e_mask]\n",
    "        exp_CPs = [cp for e_mask, cp in zip(exp_mask, CPs) if e_mask]\n",
    "        ### Collect all segments for this experiment ###\n",
    "\n",
    "        ### create the correct file structure ###\n",
    "        results_dir_path = os.getcwd() + f'/{output_name}/track_2/exp_{exp}/'\n",
    "        ### /create the correct file structure ###    \n",
    "\n",
    "        ### If needed, do ensemble level analysis ### \n",
    "        if predict_ensemble_properties:\n",
    "            ### Ensemble Level Analysis! ###\n",
    "            flat_traj_labels = [np.concatenate(exp_lab, axis=0) for exp_lab in exp_Labels]\n",
    "            flat_exp_labels = np.concatenate(flat_traj_labels, axis=0)\n",
    "            ALL_EXP_LABELS = ALL_EXP_LABELS + [flat_exp_labels]\n",
    "            \n",
    "            num_components, means, stds, weights, GMM_model = AnalyseEnsembleProperties(flat_exp_labels[:,:3])    # only consider the alpha K and diff type\n",
    "            flat_exp_model = np.mean(flat_exp_labels[3:])\n",
    "            exp_model = GetModel(flat_exp_model)\n",
    "\n",
    "            ### write to file ###\n",
    "            Path(results_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "            file = open(results_dir_path + f'ensemble_labels.txt', 'w')\n",
    "            prediction_string = f'model: {exp_model}; num_state: {num_components} \\n'\n",
    "            prediction_string += \"; \".join(means[:,0].astype('str')) + '\\n'    # all alpha means\n",
    "            prediction_string += \"; \".join(stds[:,0].astype('str')) + '\\n'    # all alpha stds\n",
    "            prediction_string += \"; \".join(means[:,1].astype('str')) + '\\n'    # all K means\n",
    "            prediction_string += \"; \".join(stds[:,1].astype('str')) + '\\n'    # all K stds\n",
    "            prediction_string += \"; \".join(weights.astype('str'))    # weights\n",
    "            file.write(prediction_string)\n",
    "            file.close()\n",
    "            ### write to file ###  \n",
    "        ## If needed, do ensemble level analysis ###\n",
    "\n",
    "\n",
    "        ## loop over all fovs in this experiment and write their info to different files ###\n",
    "        fovs = np.unique(exp_Traj_Addresses[:,1])\n",
    "        for fov in fovs:\n",
    "            ### collect all info for this FOV ###\n",
    "            fov_mask = exp_Traj_Addresses[:,1] == fov\n",
    "            fov_Labels = [seg_lab for f_mask, seg_lab in zip(fov_mask, exp_Labels) if f_mask]\n",
    "            fov_CPs = [cp for f_mask, cp in zip(fov_mask, exp_CPs) if f_mask]\n",
    "            ### /collect all info for this FOV ###\n",
    "\n",
    "            ### write to file ###\n",
    "            Path(results_dir_path).mkdir(parents=True, exist_ok=True)    # make parent dir if needed\n",
    "            file = open(results_dir_path + f'fov_{fov}.txt', 'w')\n",
    "            for traj_idx, (Traj_Labels, Traj_CPs) in enumerate(zip(fov_Labels, fov_CPs)):\n",
    "                prediction_string = str(traj_idx)\n",
    "                for seg_label, cp in zip(Traj_Labels, Traj_CPs):\n",
    "                    seg_label =  FixPredictions(np.mean(seg_label, axis=0))    # convert TS wise prediction to single values\n",
    "                    prediction_string = (prediction_string + ','  \n",
    "                                        +str(seg_label[1]) + ','    # Ks \n",
    "                                        +str(seg_label[0]) + ','    # alphas\n",
    "                                        +str(seg_label[2]) + ','    # Ms\n",
    "                                        +str(cp))\n",
    "                prediction_string = prediction_string + '\\n'\n",
    "                file.write(prediction_string)\n",
    "            file.close()\n",
    "            # /write to file ###\n",
    "        ## loop over all fovs in this experiment and write their info to diff files ###     \n",
    "\n",
    "    if predict_ensemble_properties:\n",
    "        return ALL_EXP_LABELS#, ALL_EXP_MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec946ac-c0d1-4dde-8fcb-7db6a689f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetModels(exp_model, threshold=0.0):\n",
    "    '''\n",
    "    Convert one hot M prediction to model label\n",
    "    '''   \n",
    "    models_possible = ['single_state', 'multi_state', 'dimerization', 'confinement', 'immobile_traps']\n",
    "    sorted_model_idxs = np.argsort(exp_model)[::-1]\n",
    "    sorted_model_exps = np.array([exp_model[s_m_idx] for s_m_idx in sorted_model_idxs])\n",
    "    diff_sorted_models = np.diff(-sorted_model_exps)\n",
    "    diff_sorted_models_above_threshold = diff_sorted_models > threshold\n",
    "    diff_sorted_models_above_threshold = np.concatenate([diff_sorted_models_above_threshold, [True]])\n",
    "    diff_sorted_models_above_threshold_idx = np.argwhere(diff_sorted_models_above_threshold)[0][0]+1\n",
    "    model_idxs = sorted_model_idxs[:diff_sorted_models_above_threshold_idx]\n",
    "\n",
    "    return [models_possible[m_idx] for m_idx in model_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583424c5-13c9-4845-b928-48b60adba66f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b101bde3-f4c8-48bc-9770-89766d764f97",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Build the Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bd7620-c57d-4667-848b-6dc5f6ccc17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from UNet3P_var_M2 import *\n",
    "from UNet_Blocks import *\n",
    "from utils import DiffTrajs\n",
    "\n",
    "# Make the model\n",
    "max_traj_len = 224\n",
    "filters = [16, 32, 64, 64, 128, 128]\n",
    "\n",
    "ConvBlockParams = {'num_filters': 128,\n",
    "                   'kernel_size': 3,\n",
    "                   'strides': 1,\n",
    "                   'padding': 'same'}\n",
    "\n",
    "SkipBlockParams = {'num_filters': 512,\n",
    "                   'kernel_size': 3,\n",
    "                   'strides': 1,\n",
    "                   'padding': 'same'}\n",
    "\n",
    "DecoderBlockParams = {'num_filters': 512,\n",
    "                      'kernel_size' :3,\n",
    "                      'strides': 1,\n",
    "                      'padding': \"same\"}\n",
    "\n",
    "model = UNet3P_var_M(filters, ConvBlockSimple, ConvBlockParams, SkipBlockParams, DecoderBlockParams, input_len=max_traj_len)\n",
    "\n",
    "# Set file to save to\n",
    "path = sys.path[0][:-10] + 'ChallengeNets/GeneralistNet/Model.weights.h5'\n",
    "model.load_weights(path)\n",
    "print('Network weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4e535-da68-4c1f-ba3f-7371b63cf7f0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Apply Network to Local Challenge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01753f7e-6533-4b09-a3ff-f7230bad0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/cs-solomon.asghar/AnDi_2024/public_data_challenge_v0/'\n",
    "output_name_template = \"/GeneralistNet_Predictions/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f9e28-a4e7-4a67-9058-b9eecfbbae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_Traj_Addresses, Label_Segments, All_CPs, num_exps, num_fovs = PhaseOnePredictions(model, data_path, max_traj_len=max_traj_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4953c5b-6cff-4f7b-8ab0-6182688eacf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_name_nosegnet = output_name_template\n",
    "\n",
    "All_Traj_Addresses_a = All_Traj_Addresses.copy()\n",
    "Label_Segments_a = Label_Segments.copy()\n",
    "All_CPs_a = All_CPs.copy()\n",
    "\n",
    "labs = PhaseTwoPredictions(All_Traj_Addresses_a, Label_Segments_a, All_CPs_a, num_exps, num_fovs,\n",
    "                           output_name=output_name_nosegnet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
